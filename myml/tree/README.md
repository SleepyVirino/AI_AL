# 决策树

```html
Author: 杜耀达
Email: 	d837791568yd@gmail.com
```

## 树形决策过程

1. **决策节点**：在这些节点需要进行相应的判断来决定进入哪一个分支

   - **离散属性**：根据不同的属性值进入不同的分支
   - **连续属性**：可以设置一个或多个阈值，用于将属性分段，根据该属性位于不同的区间进入不同的分支

   对于决策节点，判断规则是十分自由的，甚至可以训练相关的神经网络用来进行判断

2. **叶子节点**：表示最终的决策结果，不存在子节点。

   - **分类树**：存储类别标签
   - **回归树**：存储回归值

决策树是一种判别模型（直接对后验概率进行建模），天然支持多分类问题。典型的决策树有**ID3**、**C4.5**、**CART(分类树与回归树)**

## 训练算法

### 1. 递归分裂过程

1. 使用样本集$D$建立根节点，根据某种判定规则，将样本集分裂成$D_1,D_2,...,D_v$共$v$个部分

   - 离散属性：$v$为该属性所能取到的值的个数

   - 连续属性：设置$v-1$个阈值$t_1,t_2,...,t_{v-1}$，$t_0=-\infty,t_v=+\infty$定义判定的表达式$f(\mathbf{x})$
     $$
     D_i = \{\mathbf{x}|t_{i-1}<=f(\mathbf{x})<t_i\}
     其中i=1,2,...,n
     $$
     

     - $f(\mathbf{x})=x_k$将单个属性$x_k$分为$v$个区间，样本集也对应分为$v$部分，此时的判定规则即为使用若干平行于空间坐标轴（属性值）的平面对样本空间进行分割。
     - 可以设置多个属性的线性表达式（非线性也可以，也可以用神经网络等），可以产生不平行于坐标轴的分割平面（曲面）

2. 寻找最佳分裂$\hat{D_1},\hat{D_2},...,\hat{D_v}$

   - **信息增益(ID3)**

     当前集合$D$中第$k$类样本所占的比例为$p_k$，D的**信息熵**定义为：
     $$
     Ent(D)=-\sum_{k=1}^{|Y|}p_k log_2 p_k
     $$
     $Ent(D)$的值越小，$D$的纯度越高。

     根据判定规则$f(\mathbf{x})$和阈值$\{t_1,t_2,...,t_{v-1}\}$**信息增益**定义为
     $$
     Gain(D, f, t)=Ent(D)-\sum_{v=1}^{V}\frac{|D^v|}{|D|}Ent(D^v)
     $$
     我们的目标就是求出
     $$
     \underset{f,t}{argmax}Gain(D,f,t)
     $$

   - **信息增益率(C4.5)**
     $$
     Gain\_ratio(D,f,t)=\frac{Gain(D,f,t)}{IV(D,f,t)}
     $$
     其中
     $$
     IV(D,f,t)=-\sum_{v=1}^{V}\frac{|D^v|}{|D|} log_2 \frac{|D^v|}{|D|}
     $$
     我们只需求：
     $$
     \underset{f,t}{argmax}Gain\_ratio(D,f,t)
     $$
     

   - **基尼指数(CART)**

     **基尼值**定义为
     $$
     Gini(D)=\sum_{k=1}^{|Y|}\sum_{\hat{k}\neq k}p_kp_{\hat{k}}
     \\=1-\sum_{k=1}^{|Y|}p_k^2
     $$
     **基尼指数**定义为
     $$
     Gini\_index(D,f,t)=\sum_{v=1}^{|V|}\frac{|D^v|}{|D|}Gini(D^v)
     $$
     基尼值反映了从数据集$D$中随机抽取两个样本，类别标记不一致的概率。我们只需求：
     $$
     \underset{f,t}{argmin}Gini\_index(D,f,t)
     $$

   - **回归误差**

     上面三个评判标准均是针对**分类树**，对于回归树，我们使用均方误差进行评判，定义如下：
     $$
     E(D) = \frac{1}{l}\sum_{i=1}^{l}(y_i-\overline{y})^2
     $$
     均方误差的变化量：
     $$
     \Delta E(D,f,t) = E(D)-\sum_{v=1}^{|V|}\frac{|D^v|}{|D|}E(D^v)
     $$
     我们只需求：
     $$
     \underset{f,t}{argmax}\Delta E(D,f,t)
     $$

   - 

   对于上面求$argmax$或$argmin$的问题：

   我们举简单的例子：$f(\mathbf{x})=x_k$，$v=2$，则这是我们遍历所有的属性可能所取得所有值来确定$f(\mathbf{x}),t_1$，来求解问题即可

3. 对样本集$\hat{D_1},\hat{D_2},...,\hat{D_v}$进行递归建立子树

4. 如果不能再进行分裂，则把该节点标记为叶子节点，并存储决策结果

   不能再分裂的情况：

   - 样本集中所有样本类别标签或回归值一致，无需划分

     - 分类：决策结果为样本标签
     - 回归：决策结果为回归值

   - 样本集中属性为空（有缺失值）或样本集中所有样本属性值相同，无法划分

     - 分类：决策结果为最多的样本标签
     - 回归：决策结果为回归值均值

   - 样本集为空

     与父节点先验分布一致

     - 分类：决策结果为最多的样本标签（父节点中）
     - 回归：决策结果为回归值均值（父结点中）

   - 达到最大递归深度

     - 分类：决策结果为最多的样本标签
     - 回归：决策结果为回归值均值

   - 剪枝

     - 分类：决策结果为最多的样本标签
     - 回归：决策结果为回归值均值

## 2. 剪枝

剪枝策略主要用来对付过拟合问题，防止决策树学习的“太好”，剪枝手段主要分为两种：

1. **预剪枝**：对每个节点在划分前进行估计，如果当前节点划分后不能提升决策树的泛化能力（对验证集进行决策，观察效果），则直接标记为叶子节点，本质上是一种“贪心”算法，可能会出现欠拟合。
2. **后剪枝**：先从训练集生成一棵完整的决策树，然后按照一定的剪枝算法来进行剪枝。常见的实现方法有**降低错误剪枝(REP)**、**悲观错误剪枝(PEP)**、**代价-复杂度剪枝(CCP)**等方案。

## 3. 缺失值处理

- 将属性值缺失的样本剔除
- 每个节点有多个次分裂规则
- 对样本$\mathbf{x}$赋予权重$w_\mathbf{x}$，数值在$0\sim1$之间，然后对于原来在个数上的相关量的计算变为在权重上的计算即可。对于缺失值的数据，训练时会进入全部的分支中，但是会按照不同分支的样本个数分配不同的权重（和为1）

## 



